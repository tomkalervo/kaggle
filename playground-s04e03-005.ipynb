{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3eab746",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T07:33:57.842469Z",
     "iopub.status.busy": "2024-03-17T07:33:57.841999Z",
     "iopub.status.idle": "2024-03-17T07:33:59.030492Z",
     "shell.execute_reply": "2024-03-17T07:33:59.029260Z"
    },
    "papermill": {
     "duration": 1.19694,
     "end_time": "2024-03-17T07:33:59.033310",
     "exception": false,
     "start_time": "2024-03-17T07:33:57.836370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "train_data = pd.read_csv('/kaggle/input/playground-series-s4e3/train.csv', index_col='id')\n",
    "test_data = pd.read_csv('/kaggle/input/playground-series-s4e3/test.csv', index_col='id')\n",
    "\n",
    "# Seperate X,y\n",
    "prediction_categories = train_data.columns.drop(test_data.columns)\n",
    "X_full = train_data.drop(prediction_categories, axis=1)\n",
    "y_full = train_data[prediction_categories].copy()\n",
    "assert(X_full.shape[1] == test_data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9503999e",
   "metadata": {
    "papermill": {
     "duration": 0.002649,
     "end_time": "2024-03-17T07:33:59.039144",
     "exception": false,
     "start_time": "2024-03-17T07:33:59.036495",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Testing pytorch with different hidden activations\n",
    "The goal is to try out pytorch and to see if we get any other results using other activation functions, such as tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46a26f93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T07:33:59.046487Z",
     "iopub.status.busy": "2024-03-17T07:33:59.046055Z",
     "iopub.status.idle": "2024-03-17T07:34:00.596312Z",
     "shell.execute_reply": "2024-03-17T07:34:00.595068Z"
    },
    "papermill": {
     "duration": 1.557132,
     "end_time": "2024-03-17T07:34:00.599062",
     "exception": false,
     "start_time": "2024-03-17T07:33:59.041930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape=(15375, 27), \n",
      "X_train.dtypes=X_Minimum                  int64\n",
      "X_Maximum                  int64\n",
      "Y_Minimum                  int64\n",
      "Y_Maximum                  int64\n",
      "Pixels_Areas               int64\n",
      "X_Perimeter                int64\n",
      "Y_Perimeter                int64\n",
      "Sum_of_Luminosity          int64\n",
      "Minimum_of_Luminosity      int64\n",
      "Maximum_of_Luminosity      int64\n",
      "Length_of_Conveyer         int64\n",
      "TypeOfSteel_A300           int64\n",
      "TypeOfSteel_A400           int64\n",
      "Steel_Plate_Thickness      int64\n",
      "Edges_Index              float64\n",
      "Empty_Index              float64\n",
      "Square_Index             float64\n",
      "Outside_X_Index          float64\n",
      "Edges_X_Index            float64\n",
      "Edges_Y_Index            float64\n",
      "Outside_Global_Index     float64\n",
      "LogOfAreas               float64\n",
      "Log_X_Index              float64\n",
      "Log_Y_Index              float64\n",
      "Orientation_Index        float64\n",
      "Luminosity_Index         float64\n",
      "SigmoidOfAreas           float64\n",
      "dtype: object\n",
      "y_train.shape=(15375, 7), \n",
      "y_train.dtypes=Pastry          int64\n",
      "Z_Scratch       int64\n",
      "K_Scatch        int64\n",
      "Stains          int64\n",
      "Dirtiness       int64\n",
      "Bumps           int64\n",
      "Other_Faults    int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Break off validation set from training data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_full, y_full, train_size=0.8, test_size=0.2)\n",
    "X_test = test_data.copy()\n",
    "print(f\"{X_train.shape=}, \\n{X_train.dtypes=}\")\n",
    "print(f\"{y_train.shape=}, \\n{y_train.dtypes=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49128bfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T07:34:00.607922Z",
     "iopub.status.busy": "2024-03-17T07:34:00.606712Z",
     "iopub.status.idle": "2024-03-17T07:34:17.953777Z",
     "shell.execute_reply": "2024-03-17T07:34:17.952136Z"
    },
    "papermill": {
     "duration": 17.354858,
     "end_time": "2024-03-17T07:34:17.957068",
     "exception": false,
     "start_time": "2024-03-17T07:34:00.602210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/12], Validation Loss: 1.0559\n",
      "Epoch [2/12], Validation Loss: 1.0375\n",
      "Epoch [3/12], Validation Loss: 1.0282\n",
      "Epoch [4/12], Validation Loss: 1.0266\n",
      "Epoch [5/12], Validation Loss: 1.0200\n",
      "Epoch [6/12], Validation Loss: 1.0153\n",
      "Epoch [7/12], Validation Loss: 1.0208\n",
      "Epoch [8/12], Validation Loss: 1.0101\n",
      "Epoch [9/12], Validation Loss: 1.0159\n",
      "Epoch [10/12], Validation Loss: 1.0140\n",
      "Epoch [11/12], Validation Loss: 1.0102\n",
      "Epoch [12/12], Validation Loss: 1.0076\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert pandas DataFrame to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "\n",
    "X_valid_tensor = torch.tensor(X_valid.values, dtype=torch.float32)\n",
    "y_valid_tensor = torch.tensor(y_valid.values, dtype=torch.float32)\n",
    "\n",
    "# Define neural network architecture\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),  # Batch normalization layer after the first linear layer\n",
    "            nn.Linear(128, 64, bias=False),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "        self.bn_input = nn.BatchNorm1d(input_dim) # Batch normalization layer for input data\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.bn_input(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Define model, loss function, and optimizer\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = y_train.shape[1]\n",
    "model = SimpleNN(input_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()  # Use appropriate loss function depending on your task\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Create DataLoader for training and validation data\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 12\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_X)\n",
    "        loss = criterion(output, torch.argmax(batch_y, dim=1))  # Adjust according to your task\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "        for batch_X, batch_y in valid_loader:\n",
    "            output = model(batch_X)\n",
    "            loss = criterion(output, torch.argmax(batch_y, dim=1))  # Adjust according to your task\n",
    "            total_loss += loss.item() * batch_X.size(0)\n",
    "            total_samples += batch_X.size(0)\n",
    "        average_loss = total_loss / total_samples\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "024fbeee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T07:34:17.968786Z",
     "iopub.status.busy": "2024-03-17T07:34:17.967821Z",
     "iopub.status.idle": "2024-03-17T07:34:18.187077Z",
     "shell.execute_reply": "2024-03-17T07:34:18.185693Z"
    },
    "papermill": {
     "duration": 0.228579,
     "end_time": "2024-03-17T07:34:18.190240",
     "exception": false,
     "start_time": "2024-03-17T07:34:17.961661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities_df[:5]=         Pastry  Z_Scratch  K_Scatch    Stains  Dirtiness     Bumps  \\\n",
      "id                                                                    \n",
      "19219  0.606655   0.001112  0.000345  0.000056   0.021884  0.109314   \n",
      "19220  0.383565   0.021164  0.010911  0.000305   0.077077  0.149133   \n",
      "19221  0.085073   0.027387  0.053516  0.003135   0.011266  0.348675   \n",
      "19222  0.189699   0.003550  0.000191  0.000329   0.018666  0.263804   \n",
      "19223  0.049273   0.001380  0.000406  0.000152   0.008797  0.545828   \n",
      "\n",
      "       Other_Faults  \n",
      "id                   \n",
      "19219      0.260635  \n",
      "19220      0.357844  \n",
      "19221      0.470948  \n",
      "19222      0.523761  \n",
      "19223      0.394164  \n"
     ]
    }
   ],
   "source": [
    "# Convert test data to tensor\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "\n",
    "# Make predictions\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test_tensor)\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "probabilities = softmax(predictions)\n",
    "\n",
    "# Convert predictions tensor to numpy array\n",
    "probabilities_numpy = probabilities.numpy()\n",
    "\n",
    "# Convert numpy array to DataFrame with the same index as X_test\n",
    "probabilities_df = pd.DataFrame(probabilities_numpy, index=X_test.index, columns=y_train.columns)\n",
    "print(f\"{probabilities_df[:5]=}\")\n",
    "# Save predictions to CSV\n",
    "probabilities_df.to_csv('predictions.csv')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7659021,
     "sourceId": 68699,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 25.413591,
   "end_time": "2024-03-17T07:34:19.721284",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-17T07:33:54.307693",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
